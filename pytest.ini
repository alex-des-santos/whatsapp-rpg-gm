[tool:pytest]

# ==========================================
# CONFIGURAÇÕES BÁSICAS
# ==========================================

# Diretórios de teste
testpaths = tests

# Padrões de arquivo de teste
python_files = test_*.py *_test.py

# Padrões de função de teste
python_functions = test_*

# Padrões de classe de teste
python_classes = Test*

# ==========================================
# OPÇÕES DE EXECUÇÃO
# ==========================================

# Verbosidade aumentada
addopts = 
    --verbose
    --strict-markers
    --strict-config
    --tb=short
    --cov=app
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --cov-report=xml
    --cov-fail-under=80
    --cov-branch
    --junit-xml=test-results/junit.xml
    --html=test-results/report.html
    --self-contained-html

# Diretório mínimo para cobertura
minversion = 7.0

# ==========================================
# MARKERS PERSONALIZADOS
# ==========================================

markers =
    # Markers por tipo de teste
    unit: marca testes unitários
    integration: marca testes de integração
    e2e: marca testes end-to-end
    api: marca testes de API
    database: marca testes que usam banco de dados
    
    # Markers por componente
    character: testes relacionados a personagens
    whatsapp: testes de integração WhatsApp
    ai: testes de funcionalidades de IA
    auth: testes de autenticação
    health: testes de health checks
    
    # Markers por velocidade
    slow: marca testes lentos
    fast: marca testes rápidos
    
    # Markers por ambiente
    dev: testes para desenvolvimento
    staging: testes para staging
    prod: testes para produção
    
    # Markers especiais
    skip_on_ci: pula testes no CI
    requires_internet: testes que precisam de internet
    requires_api_keys: testes que precisam de chaves de API

# ==========================================
# CONFIGURAÇÕES DE FILTRO
# ==========================================

# Avisos ignorados
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning

# ==========================================
# CONFIGURAÇÕES DE LOGGING
# ==========================================

# Captura de logs
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Logs em arquivo
log_file = logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# ==========================================
# CONFIGURAÇÕES ASYNCIO
# ==========================================

# Modo asyncio
asyncio_mode = auto

# ==========================================
# CONFIGURAÇÕES DE TIMEOUT
# ==========================================

# Timeout global para testes
timeout = 300

# Timeout para testes marcados como lentos
timeout_method = thread

# ==========================================
# CONFIGURAÇÕES DE DISTRIBUIÇÃO
# ==========================================

# Para pytest-xdist (testes paralelos)
# Número de workers automático: -n auto
# Número fixo de workers: -n 4

# ==========================================
# CONFIGURAÇÕES DE MOCK
# ==========================================

# Configurações para mocks automáticos
mock_use_standalone_module = true

# ==========================================
# CONFIGURAÇÕES DE FIXTURES
# ==========================================

# Escopo padrão de fixtures
# function, class, module, package, session

# ==========================================
# CONFIGURAÇÕES DE COBERTURA
# ==========================================

# Arquivos para ignorar na cobertura
# Configurado via .coveragerc

# ==========================================
# CONFIGURAÇÕES DE BENCHMARK
# ==========================================

# Para pytest-benchmark
benchmark_min_rounds = 5
benchmark_max_time = 0.5
benchmark_min_time = 0.000005
benchmark_timer = time.perf_counter
benchmark_disable_gc = false
benchmark_sort = mean

# ==========================================
# CONFIGURAÇÕES DE RELATÓRIOS
# ==========================================

# Formato de output JUnit XML
junit_suite_name = whatsapp_rpg_gm
junit_logging = all
junit_log_passing_tests = true
junit_duration_report = total

# ==========================================
# CONFIGURAÇÕES ESPECÍFICAS
# ==========================================

# Para testes de banco de dados
database_url = sqlite:///test.db

# Para testes de API
api_base_url = http://localhost:8000

# Para testes de WhatsApp (mock)
whatsapp_mock_enabled = true

# ==========================================
# CONFIGURAÇÕES DE PLUGINS
# ==========================================

# Plugins habilitados
required_plugins = 
    pytest-cov>=4.0.0
    pytest-asyncio>=0.21.0
    pytest-mock>=3.10.0
    pytest-html>=3.1.0

# ==========================================
# CONFIGURAÇÕES DE DEBUGGING
# ==========================================

# Para debugging com PDB
# Use: pytest --pdb
# Para debugging automático em falhas: pytest --pdb-trace

# ==========================================
# CONFIGURAÇÕES DE COLETA
# ==========================================

# Evitar coleta de arquivos específicos
collect_ignore = [
    "setup.py",
    "conftest.py",
    "migrations/",
    "alembic/",
]

# ==========================================
# CONFIGURAÇÕES DE PERFORMANCE
# ==========================================

# Cache de pytest
cache_dir = .pytest_cache

# ==========================================
# EXEMPLOS DE USO
# ==========================================

# Executar apenas testes unitários:
# pytest -m unit

# Executar apenas testes de personagem:
# pytest -m character

# Executar testes em paralelo:
# pytest -n auto

# Executar com cobertura detalhada:
# pytest --cov=app --cov-report=html

# Executar apenas testes rápidos:
# pytest -m "not slow"

# Executar testes com debugging:
# pytest --pdb --capture=no

# ==========================================
# NOTAS
# ==========================================

# - Configure variáveis de ambiente em .env.test
# - Use fixtures para dados de teste consistentes
# - Marque testes adequadamente para facilitar execução seletiva
# - Mantenha cobertura acima de 80%
# - Use mocks para serviços externos